# 2025-NLP

- [ ]  Words Representation
- [ ]  Sequence Modeling with RNNs
- [ ]  Sequence Modeling with Transformers
- [ ]  Instruction Tuning
- [ ]  Preference Optimization
- [ ]  Build NLP Projects with Hugging Face
- [ ]  Prompting
- [ ]  Parameter Efficient Fine-Tuning
- [ ]  Distillation, Quantization, Pruning
- [ ]  Mixture of Experts, Retrieval-Augmented Generation

### ğŸ“š **NLP ê°œë… ê³µë¶€ë¥¼ ìœ„í•œ ì¶”ì²œ ìë£Œ**

âœ… **Words Representation (ë‹¨ì–´ í‘œí˜„)**

- ğŸ“– *Speech and Language Processing* - Jurafsky & Martin (ì±•í„° 6: ë‹¨ì–´ í‘œí˜„)
- ğŸ“ Word2Vec, GloVe ê´€ë ¨ ë…¼ë¬¸:
    - "Efficient Estimation of Word Representations in Vector Space" (Mikolov et al.)
    - "GloVe: Global Vectors for Word Representation" (Pennington et al.)
- ğŸ« [CS224N Stanford NLP ê°•ì˜ - Word Vectors](https://www.youtube.com/watch?v=ERibwqs9p38)

âœ… **Sequence Modeling with RNNs**

- ğŸ“– *Dive into Deep Learning* (ì±•í„° 9: ìˆœí™˜ ì‹ ê²½ë§)
- ğŸ« [CS231n RNN ê°•ì˜](https://www.youtube.com/watch?v=9zhrxE5PQgY)
- ğŸ’» PyTorch ê³µì‹ íŠœí† ë¦¬ì–¼: RNNì„ ì´ìš©í•œ NLP

âœ… **Sequence Modeling with Transformers**

- ğŸ“– *Attention Is All You Need* ë…¼ë¬¸ (Vaswani et al.)
- ğŸ« [Stanford CS224N - Transformers](https://www.youtube.com/watch?v=iDulhoQ2pro)
- ğŸ’» PyTorch ê³µì‹ Transformer íŠœí† ë¦¬ì–¼: Transformer ëª¨ë¸ êµ¬í˜„

âœ… **Instruction Tuning & Preference Optimization**

- ğŸ“– InstructGPT ë…¼ë¬¸: ["Training language models to follow instructions"](https://arxiv.org/abs/2203.02155)
- ğŸ“– RLHF (Reinforcement Learning from Human Feedback) ë…¼ë¬¸: ["Deep Reinforcement Learning from Human Preferences"](https://arxiv.org/abs/1706.03741)
- ğŸ’» OpenAIì˜ RLHF íŠœí† ë¦¬ì–¼: [Reinforcement Learning with Human Feedback](https://openai.com/research/instruction-following)

âœ… **Build NLP Projects with Hugging Face**

- ğŸ“– *Natural Language Processing with Transformers* (Lewis Tunstall, Leandro von Werra, Thomas Wolf)
- ğŸ« Hugging Face ê³µì‹ ì½”ìŠ¤
- ğŸ’» ì‹¤ìŠµ: Hugging Face `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©í•˜ì—¬ BERT, GPT ëª¨ë¸ í™œìš©

âœ… **Prompting & Parameter Efficient Fine-Tuning**

- ğŸ“– [Prompt Engineering Guide](https://www.promptingguide.ai/)
- ğŸ« LoRA, Adapter ê´€ë ¨ ë…¼ë¬¸:
    - ["LoRA: Low-Rank Adaptation of Large Language Models"](https://arxiv.org/abs/2106.09685)
    - ["Adapters: Efficient Transfer Learning"](https://arxiv.org/abs/1902.00751)
- ğŸ’» ì‹¤ìŠµ: Hugging Face `peft` ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©í•˜ì—¬ LoRA ì ìš©

âœ… **Distillation, Quantization, Pruning**

- ğŸ“– *Model Compression Techniques* ë…¼ë¬¸
- ğŸ« [DistilBERT ë…¼ë¬¸](https://arxiv.org/abs/1910.01108)
- ğŸ’» ì‹¤ìŠµ:
    - `torch.quantization`ì„ ì‚¬ìš©í•œ ëª¨ë¸ ê²½ëŸ‰í™”
    - `distilbert` ë° `TinyBERT`ë¥¼ í™œìš©í•œ ê²½ëŸ‰ NLP ëª¨ë¸ ì‹¤í—˜

âœ… **Mixture of Experts (MoE), Retrieval-Augmented Generation (RAG)**

- ğŸ“– ["GLaM: Efficient Scaling of Large Language Models"](https://arxiv.org/abs/2112.06905)
- ğŸ“– ["Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"](https://arxiv.org/abs/2005.11401)
- ğŸ’» ì‹¤ìŠµ:
    - Hugging Face `rag-token`ì„ í™œìš©í•œ RAG ëª¨ë¸ êµ¬í˜„
    - `Switch Transformer`ë‚˜ `GLaM` ë…¼ë¬¸ êµ¬í˜„ ì½”ë“œ ë¶„ì„
 

Lab 1: Minimal Byte Pair Encoding (10pt)

Lab 3: Nano GPT (10pt)

Lab 4: Building Your NLP Projects (10pt)

Lab 5: Parameter Efficient Fine-Tuning (10pt)

### ğŸ›  **Lab ì‹¤ìŠµ ë°©ë²• ê°€ì´ë“œ**

### **Lab 1: Minimal Byte Pair Encoding (BPE)**

- ëª©í‘œ: BPE ì•Œê³ ë¦¬ì¦˜ì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ í† í°í™”ë¥¼ ì‹¤í—˜
- ì‹¤ìŠµ:
    - ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ BPE ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (Python, NumPy í™œìš©)
    - `sentencepiece` ë° `Hugging Face tokenizers`ë¥¼ ì´ìš©í•œ BPE ì ìš©

### **Lab 3: NanoGPT**

- ëª©í‘œ: ê°„ë‹¨í•œ GPT ëª¨ë¸ì„ ì§ì ‘ í•™ìŠµì‹œí‚¤ê³  ë™ì‘ ë°©ì‹ ì´í•´
- ì‹¤ìŠµ:
    - `karpathy/nanoGPT` ì½”ë“œ ë¶„ì„ ë° ì‹¤í–‰
    - ì‘ì€ ë°ì´í„°ì…‹ (ì˜ˆ: Shakespeare text)ìœ¼ë¡œ GPT í•™ìŠµ ì‹¤í—˜
    - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤ìŠµ (í† í° í¬ê¸°, ë ˆì´ì–´ ìˆ˜ ì¡°ì • ë“±)

### **Lab 4: Building Your NLP Projects**

- ëª©í‘œ: NLP í”„ë¡œì íŠ¸ ê¸°íš ë° ë°ì´í„°ì…‹ êµ¬ì¶•
- ì‹¤ìŠµ:
    - Hugging Face `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ ë°ì´í„°ì…‹ ì •ì œ
    - `transformers`ë¥¼ í™œìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì ìš©
    - íŠ¹ì • NLP íƒœìŠ¤í¬ (ì˜ˆ: ê°ì„± ë¶„ì„, ìš”ì•½) ìˆ˜í–‰

### **Lab 5: Parameter Efficient Fine-Tuning (PEFT)**

- ëª©í‘œ: LoRA, Adapter ë“±ì„ ì‚¬ìš©í•œ íš¨ìœ¨ì ì¸ ëª¨ë¸ íŠœë‹
- ì‹¤ìŠµ:
    - Hugging Face `peft` ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©
    - ê¸°ì¡´ GPT ëª¨ë¸ì— LoRA ì ìš© í›„ ì„±ëŠ¥ ë¹„êµ
    - ë°ì´í„°ì…‹ í¬ê¸°ë³„ Fine-tuning ì‹¤í—˜
