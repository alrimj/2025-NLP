# NLP의 기본 개념 이해
자연어 처리는 컴퓨터와 인간 언어간의 상호작용을 다루는 AI분야로 컴퓨터가 인간의 언어를 이해하고 해석하도록 하는 기술이다.
NLP의 기본 개념중 하나는 `텍스트 전처리`이다.
이후, NLP는 텍스트의 의미를 이해하기 위한 다양한 기법을 사용한다.
또한 NLP에서는 문장 구조를 이해하는 구문 분석(Syntactic Parsing)과 
문장에서 단어의 의미 및 문장 전체의 의미를 파악하는 의미 분석(Semantic Analysis)도 중요하다.

## 텍스트 전처리
원시 텍스트 데이터에는 종종 노이즈가 많고 분석하기에 적합하지 않은 상태이다. 따라서 텍스트 전처리는 불필요한 데이터를 제거하고
텍스트를 정제하여 분석 가능한 형태로 만드는 과정이라 할 수 있다. 이 과정에는 `토큰화`, `불용어 제거`, `어간 추출`, `표제어 추출` 등이 포함된다.
- 토큰화 : 문장을 단어 또는 구로 나누는 작업.
- 불용어 제거 : "과", "이","가"와 같이 자주 사용되지만 의미가 없는 단어를 제거하는 것.
- 어간 추출 및 표제어 추출 : 단어의 변형을 정규화하여 동일한 의미를 가진 단어들을 통합하는 과정.

### Step 1. 토큰화의 원리와 방법
토큰화는 NLP에서 매우 중요한 단계로, 주어진 텍스트를 의미있는 단위로 나누는 과정이다. 일반적으로 이러한 단위는 단어, 구, 문장 혹은 문자일 수 있다.
토큰화는 머신러니 및 딥러닝 모델이 언어를 이해하고 처리하는 데 필수적인 전처리 단계로, 데이터의 품질과 모델의 성능에 큰 영향을 미친다.

가장 기본적인 원리는 문장을 구성하는 의미 있는 단위로 나누는 것이다. 이 외에 토큰화에는 여러가지 방법이 있다.
대표적으로 공백 기반, 구두점 기반, 정규 표현식, 서브워드 토큰화와 같은 기법이 있다.
- 공백 기반 토큰화 : 단어 사이의 공백을 기준으로 나눔.
  - 영어와 같이 공백으로 단어를 구분하는 언어에서만 효과적.
  - 한국어와 같은 교착어에서는 문제가 발생한다. ex) "사과를"과 같은 형태소가 공백 기준으로는 단어로 인식되지 않음
- 구두점 기반 토큰화 : 문장 내의 구두점을 기준으로 토큰을 나누는 방법.
  - 문장 구조를 고려하여 문맥을 분석하는 데 유용.
  - 줄임말이나 특정 문맥에서의 구두점 사용은 토큰화에 혼란을 줌.
- 정규 표현식 : 특정 패턴을 기반으로 텍스트를 분석하는 방법으로 더 정교한 토큰화를 가능하게 한다.
  - 이메일 주소나 URL, 전화버호 같은 특정 형식을 지닌 데이터를 처리할 때 유용함.
  - 또한 더 복잡한 규칙을 적용할 수 있어 유연성을 제공한다.
- 서브워드 토큰화 : 최근에 많이 사용되는 기법으로, 대규모 언어 모델에서 효과적이다.
  - 단어를 더 작은 단위로 나누어, 희귀 다어나 새로운 단어를 효과적으로 처리할 수 있게 해준다.
  - BPE(Byte Pair Encoding)와 같은 알고리즘이 대표적이다.
    - 단어를 자주 등장하는 서브워드 단위로 나누어, 모델이 더 많은 단어를 이해할 수 있도록 한다.
    - ex) "unhappiness"에서 "un", "happi", "ness"와 같은 서브워드로 나누기.
   
토큰화 과정에서 발생 가능한 문제에는 여러가지가 있다.
1. 다의어와 동의어 처리 문제 : 문맥을 고려하지 않고 단순히 토큰화하면 의미를 제대로 전달하지 못할 수 있다.
2. 복합어 처리 문제 : 한국어와 같은 언어에서는 한 단어가 여러 형태로 변형될 수 있어 올바르게 처리하기 위해서는 형태소 분석이 필요하다.
3. 특수 문자나 이모티콘, 슬랭 등의 비표준 언어의 처리 문제 : 일반적인 토큰화 방법으로는 잘 처리되지 않아 별도의 전처리 과정이 필요할 수 있다.

### Step 2. 형태소 분석하기
형태소 : 의미를 가진 가장 작은 단위로, 단어를 구성하는 요소들이라 할 수 있다.
이것의 분석의 과정은 통상적으로 토큰화, 형태소 분리, 품사 태깅으로 이루어진다.
- 토큰화 : 문자열을 의미 있는 단위로 나누는 과정. (이 과정에서 문장이 단어로 나뉘고 기초 데이터가 생성됨.)
- 형태소 분리 : 주어진 문장에서 각 형태소를 식별해내는 작업.
  - 예를 들어, "나는 학교에 간다"라는 문장은 "나", "는", "학교", "에", "가", "ㄴ다"로 나눌 수 있음.
-  품사 태깅 : 각 형태소에 대해 해당하는 품사를 부여하는 과정.
  - 명사, 동사, 형용사, 부사 등으로 분류된다.
  -  "나는"은 주어를 나타내는 대명사와 조사의 결합으로 볼 수 있고, "학교"는 명사, "가"는 동사 어근, "ㄴ다"는 동사의 어미로 분석될 수 있음.

형태소 분석을 위해서는 여러 기법이 사용된다. 전통적으로는 규칙 기반의 형태소 분석기가 있으며 이는 특정 규칙에 기반해 형태소를 식별 및 품사를 태깅하는 방식이다.
이 방법은 언어의 복잡성이 증가할수록 한계가 정해져 있다. 최근에는 머신러닝 및 딥러닝 기술을 활용한 형태서 분석기가 사용되고 있으며,
이들은 대량의 데이터로 학습해 보다 정확하고 유연한 형태소 분석을 가능하게 한다. (조건부 확률 모델이나 RNN, Transformer 모델 등의 활용)

형태소 분석의 몇 가지 도전 과제들:
1. 복합어와 파생어의 처리 : 사랑+스럽다 와 같이 두 형태소를 올바르게 식별할 수 있어야 함.
2. 동음이의어 문제 : 문맥에 따라 파악해야 함.
3. 언어의 발전과 변화에 따라 형태소 분석 또한 지속적인 발전이 필요함. : 최근의 비표준 언어의 사용 증가

## Step 3. 텍스트 의미 이해하기
- Word Embedding : 단어를 고차원 공간의 벡터로 변환하여 단어의 유사성을 수치적으로 표현.
  - Word2Vec, GloVe와 같은 모델
  - 대량의 텍스트 데이터를 학습해 단어간 의미적 유사성을 반영하는 벡터를 생성한다.
 
단어 임베딩은 NLP에서 중요한 개념 중 하나로 주요 목표는 비슷한 의미를 가진 단어들이 벡터 공간에서 가까운 위치에 놓이도록 하는 것이다.
이러한 벡터 표현으로 머신러닝 알고리즘이 단어의 의미를 잘 이해하고 다양한 자연어 처리 작업에서 성능을 향상시킬 수 있게 한다.

- Word2Vec : Google에서 개발한 알고리즘으로, 두가지 모델인 CBOW(Continuous Bag of Words)와 Skip-gram을 사용한다.
  - CBOW는 주변 단어를 기반으로 중심 단어를 예측하는 방식
  - Skip-gram은 중심 단어를 기반으로 주변 단어를 예측하는 방식
- GloVe(Global Vectors for Word Representation)는 Stanford Univ에서 개발된 기법으로, 단어 간의 동시 출현 확률을 기반으로 단어 벡터를 학습한다.
  - 단어 간의 관계를 수치적으로 표현하기 위해 전체 텍스트 코퍼스에서 단어의 출현 빈도를 분석한다.
  - 이를 행렬로 구성한 후 행렬 분해를 통해 단어 벡터를 생성한다.
- FastText : Facebook에서 개발한 단어 임베딩 기법으로, 단어를 구성하는 n-그램을 사용해 단어 벡터를 생성한다.
  - 이 접근 방식은 특히 희귀 단어에 대한 임베딩을 생성하는 데 유리하다.
  - ex) 불가능이란 단어의 불과 가능이라는 두 부분으로 나누어 벡터를 생성할 수 있다.

### Transformer 구조의 이해
BERT나 GPT같이 '트랜스포머 기반의  모델'들이 등장하며 문맥을 고려한 단어 임베딩이 가능해지며 더욱 정교한 자연어 처리 작업이 가능해졌다.
이렇게 Transformer는 NLP 분야에 혁신적인 모델로 자리를 잡았다. 

2017년 제안된 이 모델은 기존의 RNN이나 CNN과는 다른 방식으로 입력 데이터를 처리한다. 
기본 아이디어는 입력 시퀀스의 각 요소 간의 관계를 이해하는데 집중하는 것이다.
이를 위해 'Attention' 메커니즘을 활용해 입력 시퀀스의 모든 위치를 동시에 고려할 수 있다.

트랜스포머의 구조는 크기 인코더와 디코더로 나뉜다.
- 인코더 : 입력 문장을 받아들여 이를 잠재 공간으로 변환하는 역할을 한다.
- 디코더 : 이 잠재 공간을 기반으로 새로운 출력을 생성한다.

인코더와 디코더는 각 여러 층들로 구성되며 각 층은 동일한 구조를 가진다.
인코더의 각 층은 먼저 셀프 어텐션을 수행 후, feedforward 신경망을 통해 출력을 생성한다.
디코더는 인코더의 출력과 자신의 이전 출력을 기반으로 다음 출력을 생성하는 방식으로 작동한다.
디코더의 각 층에도 셀프 어텐션이 포함되어 있으며, 인코더의 출력에 대한 어텐션을 추가적으로 수행한다.
   => 이러한 구조로 Transformer는 병렬 처리가 가능해 학습 속도를 크게 향상.

Attention 메커니즘은 입력 데이터의 각 요소가 다른 요소와 얼마나 관련성이 있는지를 평가한다.
이 과정에서 Self Attention이 중요한 역할을 한다. 이는 입력 시퀀스의 각 단어가 다른 단어와의 관계를 고려해 가중치를 부여하는 방식으로 작동한다.
이를 통해 모델은 문맥을 반영한 표현을 생성할 수 있다. 구체적인 작동 방식은 `쿼리`,` 키`, `값`이라는 세 요소를 기반으로 한다.
입력 시퀀스의 각 단어는 쿼리, 키, 값 벡터로 변환된다. 셀프 어텐션은 쿼리와 키 벡터 간의 내적을 계산해 유사도를 측정하고 이를 softmax 함수를 통해 가중치로 변환한다.
각 값 벡터는 이 가중치에 따라 가중합을 구해 최종 출력을 생성한다. 
=> 이 방식으로 모델은 각 단어의 문맥적 의미를 포착.

추가적으로 중요한 특징은 '포지셔널 인코딩'이다. RNN과 달리 Transformer는 입력 시퀀스의 순서를 고려하지 않기 때문에, 각 단어의 위치 정보를 추가로 제공해야 한다.
이를 위해 포지셔널 인코딩으로 단어의 위치를 벡터로 표현하며 이 벡터는 입력 단어의 임베딩 벡터에 더해져 최종 입력으로 사용된다. 
포지셔널 인코딩은 사인과 코사인 함수를 사용해 각 위치에 대한 고유한 값을 생성한다.
=> 단어의 순서를 인식할 수 있게 됨.
